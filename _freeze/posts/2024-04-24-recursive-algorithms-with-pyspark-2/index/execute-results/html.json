{
  "hash": "ef9038ee3115030487b1fb5540a0228f",
  "result": {
    "markdown": "---\ntitle: A PySpark-native way to do recursion\nsubtitle: \"Part 2: Solutions\"\nauthor: corey\ndate: 2024-04-21\ncategories:\n  - data\n  - spark\n  - python\nexecute:\n    warning: false\neditor_options: \n  markdown: \n    wrap: sentence\n---\n\nIn [my last post](../2024-04-21-recursive-algorithms-with-pyspark-1/index.qmd), I described an example of recursive algorithms, the Fibonacci sequence, and showed that it can't be solved with classic SQL tools like window functions.\nIn this post, I'll explore possible solutions and demonstrate my preferred, PySpark-native approach.\n\n\n## possible solutions\n\nMost answers to this problem rely on Python user-defined functions (UDFs).\nIn Spark, a Python UDF works by:\n1. creating tiny Python sessions for each row\n2. converting the data from Scala/Java datatypes to Python data types\n3. running the Python code\n4. re-converting the data into Scala datatypes\n\nThe code might look like the example Python script above or it could be more concise, but either way, it needs to go through this 4-step process for each row, which makes even simple UDFs computationally expensive.\nPlus, adding $r$ rows to a computation running on a cluster with $w$ worker nodes scales the computational cost by $\\frac{r}{w}$.\n\nOne way to speed these computations up is to use Pandas UDFs.\nUnlike Python UDFs, which Pandas UDFs only go through this process once per *partition*.\nSo, assuming you are computing this for a bunch of grouped data, you only need to perform the 4-step process once per group.\nStill, there must be a way to do this without all that overhead, right?\n\nOne option is to use Java or Scala UDFs, which removes lots of the overhead, but of course is harder to handoff to other people who don't know Java or Scala.\nWith a problem more complex than the Fibonacci sequence, that might be a dealbreaker.\n\n## my preferred solution\n\nEnter `aggregate()` and array functions.\nYou can use these methods to calculate recursive problems using only PySpark.\nOf course, these are compiled to Scala under the hood, so you could easily see this as an intermediate step to learning a new language, but I think seeing this method made me a more creative programmer.\n\nEssentially, you'll create a vector of input data and then iterate a function over that.\nFor our Fibonacci example, this means that you'll need to create an $n \\times 1$ vector within each column, so there's a possibility of an out-of-memory error if $n$ is big enough.\n\nLet's start with a new data frame and go from there:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pyspark.sql.functions as f\nimport pyspark.sql.types as t\nfrom pyspark.sql import DataFrame, SparkSession\n\nspark = SparkSession.builder.getOrCreate()\n\ndf = spark.createDataFrame([(1, ), (2, ), (3, ), (4, ), (5, )], (\"n\", ))\n```\n:::\n\n\nWith row numbers assigned to each row, we can create base vectors\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndf = (\n  df.agg(\n    f.sort_array(f.collect_set(\"n\")).alias(\"BaseArray\")\n  )\n)\n\ndf.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+---------------+\n|      BaseArray|\n+---------------+\n|[1, 2, 3, 4, 5]|\n+---------------+\n\n```\n:::\n:::\n\n\nNow, we can start working with `aggregate`.\nLet's look at the definition of this function:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndef aggregate(\n    col: ColumnOrName,\n    initialValue: ColumnOrName,\n    merge: (Column, Column) -> Column,\n    finish: ((Column) -> Column) | None = None\n) -> Column\n```\n:::\n\n\nThis function works like a local accumulator, allowing each row to iterate over the `col`---`BaseArray` in this case---and, crucially, carry over values between iteration steps.\n\n`initialValue` is crucial for the Fibonacci sequence because it lets us set `fibonacci(0)` and `fibonacci(1)`:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nf.struct(\n  f.lit(0).alias(\"nMinus2\"),\n  f.lit(1).alias(\"nMinus1\")\n)\n```\n:::\n\n\n`merge` does the heavy lifting for this function, choosing how to get from one step to the other.\nOne annoying necessity is that it must operate on PySpark columns using PySpark column functions.\nThat means we can't write many functions using the native Python library and operators, but for the Fibonacci sequence it shouldn't be a problem.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ndef fib(previous, current):\n  n_2 = previous.nMinus1\n  n_1 = previous.nMinus1 + previous.nMinus2\n  return f.struct(n_2.alias(\"nMinus2\"), n_1.alias(\"nMinus1\"))\n```\n:::\n\n\nLast, `finish` requires another function to extract the final value.\nSince we only want the last value to calculate the fibonacci, this is very straightforward:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nlambda x: x.nMinus1\n```\n:::\n\n\n# putting it all together\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ndf = (\n  df\n  .withColumn(\n    \"Fibonacci\",\n    f.aggregate(\n      col = \"BaseArray\",\n      initialValue = f.struct(\n        f.lit(0).cast(t.LongType()).alias(\"nMinus2\"),\n        f.lit(1).cast(t.LongType()).alias(\"nMinus1\")\n      ),\n      merge = fib,\n      finish = lambda x: x.nMinus1\n    )\n  )\n)\n```\n:::\n\n\nSo, did it work?\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ndf.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+---------------+---------+\n|      BaseArray|Fibonacci|\n+---------------+---------+\n|[1, 2, 3, 4, 5]|        8|\n+---------------+---------+\n\n```\n:::\n:::\n\n\nAs you can see, this returned a single value.\nNow let's check it against the native Python implementation:\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ndef fibonacci(n: int) -> int:\n  if n == 0:\n    f = 0\n  elif n == 1:\n    f = 1\n  else:\n    f = fibonacci(n - 1) + fibonacci(n - 2)\n  return f\n\nfibonacci(6)\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\n8\n```\n:::\n:::\n\n\nYou'll notice our PySpark function doesn't actually use `current` for anything, though it's included to meet the function requirements.\nYou could make this example more complex by scaling each $fibonacci(n)$ by $n$.\nThis would change the recursive element to be $fibonacci(n) = n \\times fibonacci(n-1) + fibonacci(n-2)$.\n\nIt's a contrived example, but it illustrates how you could pull in another variable.\nThis lets us change the function above to make use of the `current` value:\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\ndef fib(previous, current):\n  n_2 = previous.nMinus1\n  n_1 = current * (previous.nMinus1 + previous.nMinus2)\n  return f.struct(n_2.alias(\"nMinus2\"), n_1.alias(\"nMinus1\"))\n```\n:::\n\n\nIn turn, this outputs:\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\n(\n    df\n    .withColumn(\n        \"ScaledFibonacci\",\n        f.aggregate(\n            col = \"BaseArray\",\n            initialValue = f.struct(\n            f.lit(0).cast(t.LongType()).alias(\"nMinus2\"),\n            f.lit(1).cast(t.LongType()).alias(\"nMinus1\")\n            ),\n            merge = fib,\n            finish = lambda x: x.nMinus1\n        )\n    )\n    .show()\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+---------------+---------+---------------+\n|      BaseArray|Fibonacci|ScaledFibonacci|\n+---------------+---------+---------------+\n|[1, 2, 3, 4, 5]|        8|            455|\n+---------------+---------+---------------+\n\n```\n:::\n:::\n\n\n## tl;dr\n\nWe can perform recursive algorithms using only native PySpark.\nThis let's us expand the types of problems we can solve while:\n1. avoid the overhead associated with Python and Pandas UDFs\n2. not assuming our collborators know any other languages\n\nI demonstrated a classic recursive problem, the Fibonacci sequence, but the same approach will work for lots of harder problems.\nFor instance, how would you implement a Poisson CDF up to the value $k$?\n\n$$\nCDF(k) = e^{-\\lambda}\\sum_{j=1}^{k}\\frac{k^j}{j!}\n$$\n\nfor $k = {0, 1, 2, \\dots}$\n\nHint: start by repeating $e^{-\\lambda}$ in a $k \\times 1$ array.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}