{
  "hash": "3f86835c89d5a2d26f5d7379abba2c36",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Evaluating Canadian tidal data\nauthor: corey\ndate: 2025-08-09\ncategories:\n  - R\n  - floatingtrails\n  - kayaking\neditor_options: \n  markdown: \n    wrap: sentence\nfrom: markdown+emoji\ndraft: false\nexecute:\n  cache: true\n  warning: false\n---\n\nA few weeks ago, I reached out to the maintainer of [Floating Trails](https://floatingtrails.com/42.36384/-71.03889/15?m.0.ll=41.51794+-70.67996&m.0.c=%2300FF00&du=n), a free website for ocean-going paddlers that allows users to view nautical charts and campsites for the US and Canadian coastline.\nI asked to help contribute and he agreed; his first interest was adding tidal predictions for the Canadian coastline.\nFloating Trails offers water-level, current speed, and current direction predictions in the US for every 15 minutes and the first task for integrating Canadian data was unerstanding whether coverage for Canadian tidal stations was comparable.\n\n# The Canadian Hydrographic Service\n\nCHS operates tidal stations in the Atlantic, Pacific, and Arctic Oceans in addition to the St. Lawrence river.\n\n::: {.column-margin}\nThe St. Lawrence, with its funnel shape and shallow depth, makes for intense tides.\n:::\nEach station include at least one of:\n\n-   buoy\n-   ground tackle\n-   current meters\n-   radio transmitter\n-   recording mechanism\n\nCHS provides a REST API for these stations and [decent documentation](https://api.iwls-sine.azure.cloud-nuage.dfo-mpo.gc.ca/swagger-ui/index.html#/) of each endpoint.\n\n# What tidal data does each station offer?\n\nWe can query metadata about stations by pinging the `stations/{stationId}/metadata` endpoint.\nI'm using `httr2` to perform and interpret each request and---of course---`tidyverse`:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(httr2)\n\nbase_url <- \"https://api.iwls-sine.azure.cloud-nuage.dfo-mpo.gc.ca\" |>\n    request() |>\n    req_url_path_append(\"api/v1\") |>\n    req_user_agent(\"floatingtrails.dev\") |>\n    req_headers(accept = \"application/json\")\n```\n:::\n\n\nUsing this API path, we can fetch metadata for the `St. Lawrence` station:\n\n\n```{.r .cell-code}\nbase_url |>\n    req_url_path_append(\"stations/5cebf1e13d0f4a073c4bbeb9/metadata\") |>\n    req_perform() |>\n    resp_body_string()\n```\n\n[1] \"{\\\"id\\\":\\\"5cebf1e13d0f4a073c4bbeb9\\\",\\\"code\\\":\\\"00755\\\",\\\"officialName\\\":\\\"St. Lawrence\\\",\\\"alternativeNames\\\":\\\"Great St. Lawrence\\\",\\\"latitude\\\":46.916789,\\\"longitude\\\":-55.39005,\\\"type\\\":\\\"PERMANENT\\\",\\\"status\\\":\\\"OK\\\",\\\"operating\\\":true,\\\"expectedProductivityPerHour\\\":60,\\\"owner\\\":\\\"CHS-SHC\\\",\\\"stationOwner\\\":{\\\"id\\\":\\\"5ce598ed487b84486892825c\\\",\\\"code\\\":\\\"CHS-SHC\\\",\\\"version\\\":1,\\\"acronymEn\\\":\\\"CHS\\\",\\\"acronymFr\\\":\\\"SHC\\\",\\\"nameEn\\\":\\\"Canadian Hydrographic Service\\\",\\\"nameFr\\\":\\\"Service hydrographique du Canada\\\"},\\\"chsRegionCode\\\":\\\"ATL\\\",\\\"provinceCode\\\":\\\"NL\\\",\\\"classCode\\\":\\\"A\\\",\\\"isTidal\\\":true,\\\"timeZoneCode\\\":\\\"Canada/Newfoundland\\\",\\\"tideTableId\\\":\\\"5da0907154c1370c6037fcce\\\",\\\"isTideTableReferencePort\\\":false,\\\"timeSeries\\\":[{\\\"id\\\":\\\"5cebf1e13d0f4a073c4bbeb5\\\",\\\"code\\\":\\\"wlo\\\",\\\"nameEn\\\":\\\"Water level official value\\\",\\\"nameFr\\\":\\\"Niveau d'eau, valeur officielle\\\",\\\"phenomenonId\\\":\\\"5ce598df487b84486892821c\\\",\\\"owner\\\":\\\"CHS-SHC\\\"},{\\\"id\\\":\\\"5cebf1e13d0f4a073c4bbeb6\\\",\\\"code\\\":\\\"wlp\\\",\\\"nameEn\\\":\\\"Water level predictions\\\",\\\"nameFr\\\":\\\"Prédictions de niveaux d'eau\\\",\\\"phenomenonId\\\":\\\"5ce598df487b84486892821c\\\",\\\"owner\\\":\\\"CHS-SHC\\\"},{\\\"id\\\":\\\"5cebf1e13d0f4a073c4bbeb7\\\",\\\"code\\\":\\\"wlf\\\",\\\"nameEn\\\":\\\"Water level forecasts generated by the FMS\\\",\\\"nameFr\\\":\\\"Prévisions de niveaux d'eau générées par le FMS\\\",\\\"phenomenonId\\\":\\\"5ce598df487b84486892821c\\\",\\\"owner\\\":\\\"CHS-SHC\\\"},{\\\"id\\\":\\\"5d9dd7cc33a9f593161c3ffc\\\",\\\"code\\\":\\\"wlp-hilo\\\",\\\"nameEn\\\":\\\"High and Low Tide Predictions\\\",\\\"nameFr\\\":\\\"Prédictions de pleines et basses mers\\\",\\\"phenomenonId\\\":\\\"5ce598df487b84486892821c\\\",\\\"owner\\\":\\\"CHS-SHC\\\"}],\\\"datums\\\":[{\\\"code\\\":\\\"CGVD28\\\",\\\"offset\\\":-1.32,\\\"offsetPrecision\\\":2},{\\\"code\\\":\\\"NAD83_CSRS\\\",\\\"offset\\\":1.43,\\\"offsetPrecision\\\":2}],\\\"heights\\\":[{\\\"heightTypeId\\\":\\\"5cec2eba3d0f4a04cc64d5d7\\\",\\\"value\\\":2.67,\\\"valuePrecision\\\":2},{\\\"heightTypeId\\\":\\\"5cec2eba3d0f4a04cc64d5d8\\\",\\\"value\\\":0.28,\\\"valuePrecision\\\":2},{\\\"heightTypeId\\\":\\\"5cec2eba3d0f4a04cc64d5ce\\\",\\\"value\\\":2.63,\\\"valuePrecision\\\":2},{\\\"heightTypeId\\\":\\\"5cec2eba3d0f4a04cc64d5d3\\\",\\\"value\\\":0.33,\\\"valuePrecision\\\":2},{\\\"heightTypeId\\\":\\\"5cec2eba3d0f4a04cc64d5d4\\\",\\\"value\\\":2.17,\\\"valuePrecision\\\":2},{\\\"heightTypeId\\\":\\\"5cec2eba3d0f4a04cc64d5d5\\\",\\\"value\\\":0.72,\\\"valuePrecision\\\":2},{\\\"heightTypeId\\\":\\\"65316afc3cf474827e39a7ef\\\",\\\"value\\\":2.07,\\\"valuePrecision\\\":2},{\\\"heightTypeId\\\":\\\"65316afc3cf474827e39a7f0\\\",\\\"value\\\":0.74,\\\"valuePrecision\\\":2},{\\\"heightTypeId\\\":\\\"5cec2eba3d0f4a04cc64d5d2\\\",\\\"value\\\":1.39,\\\"valuePrecision\\\":2},{\\\"heightTypeId\\\":\\\"5cec2eba3d0f4a04cc64d5d1\\\",\\\"value\\\":3.63,\\\"valuePrecision\\\":2,\\\"date\\\":\\\"2021-09-10\\\"},{\\\"heightTypeId\\\":\\\"5cec2eba3d0f4a04cc64d5d6\\\",\\\"value\\\":-0.4,\\\"valuePrecision\\\":1,\\\"date\\\":\\\"2021-09-11\\\"}]}\"\n\nThis fetch gets us:\n\n-   `lat`\n-   `lng`\n-   `station` (code)\n-   `id`\n-   `name`\n-   `timeZoneId`\n-   `depth`\n\nFrustratingly, [the API documentation](https://api.iwls-sine.azure.cloud-nuage.dfo-mpo.gc.ca/swagger-ui/index.html#/station%20metadata) lists fields that are important for tidal data but are not populated:\n\n-   `[highestHigh|lowestLow]WaterTimeDifference`\n-   `[flood|ebb]direction`\n\nIt turns out that currents are only predicted at a few stations---less than 10---around Canada. Those stations, such as `Abegweit Passage`, include the flood and ebb direction as well as predictions throughout the day.\n\nThe CHS API does add `tideTypeCode`, which designates the tide at that station as one of:\n\n::: {.column-margin}\nMore information at [CHS' website](https://tides.gc.ca/en/definitions-content-tides-and-currents).\n:::\n\n-   **s**emi-**d**iurnal\n-   **m**ainly **s**emi-**d**iurnal\n-   **m**ainly **d**iurnal\n-   **d**iurnal\n\nNow we know about what kind of information we can learn about each station but we don't know which stations exist!\n\n# Where are tidal stations located?\n\nAfter querying this API a few times, I noticed that some stations are not currently operating.\nTo filter these stations, I chose stations with future timestamps but without specifying any time-series codes.\n::: {.column-margin}\nEach `timeSeriesCode` corresponds to a different type of data, such as `wlp` for water-level predictions and `wlo` for water-level observations.\n:::\n\nLet's get these stations as a dataframe for further visualization:\n\n::: {.cell}\n\n```{.r .cell-code}\nall_stations <- base_url |>\n    req_url_path_append(\"stations\") |>\n    req_url_query(!!!list(\n        dateStart = \"2025-08-01T00:00:00Z\",\n        dateEnd = \"2025-08-09T23:59:59Z\"\n    )) |>\n    req_perform() |>\n    resp_body_json() |>\n    map(\\(x) as_tibble(x)) |>\n    list_rbind() |>\n    unnest_wider(timeSeries, names_sep = \"\")\n```\n:::\n\n\nThese data have abbreviated info about each station, plus information about the time series they offer:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(all_stations)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 456\nColumns: 16\n$ id                     <chr> \"5cebf1e43d0f4a073c4bc45a\", \"5cebf1e43d0f4a073c…\n$ code                   <chr> \"07780\", \"07780\", \"07780\", \"11860\", \"11860\", \"0…\n$ officialName           <chr> \"Ambleside\", \"Ambleside\", \"Ambleside\", \"Goderic…\n$ operating              <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,…\n$ latitude               <dbl> 49.32577, 49.32577, 49.32577, 43.74535, 43.7453…\n$ longitude              <dbl> -123.15458, -123.15458, -123.15458, -81.72780, …\n$ type                   <chr> \"PERMANENT\", \"PERMANENT\", \"PERMANENT\", \"PERMANE…\n$ timeSeriesid           <chr> \"5cebf1e43d0f4a073c4bc455\", \"5cebf1e43d0f4a073c…\n$ timeSeriescode         <chr> \"wlp\", \"wlo\", \"wlp-hilo\", \"wlo\", \"wlf\", \"wlp\", …\n$ timeSeriesnameEn       <chr> \"Water level predictions\", \"Water level officia…\n$ timeSeriesnameFr       <chr> \"Prédictions de niveaux d'eau\", \"Niveau d'eau, …\n$ timeSeriesphenomenonId <chr> \"5ce598df487b84486892821c\", \"5ce598df487b844868…\n$ timeSeriesowner        <chr> \"CHS-SHC\", \"CHS-SHC\", \"CHS-SHC\", \"CHS-SHC\", \"CH…\n$ timeSerieslatitude     <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ timeSerieslongitude    <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ alternativeName        <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n```\n\n\n:::\n:::\n\n\nTo see them a bit more comprehensively, we can visualize them as a tile plot:\n\n::: {.cell}\n\n```{.r .cell-code}\nall_stations |>\n    group_by(timeSeriescode) |>\n    mutate(not_null = sum(!is.na(timeSeriescode))) |>\n    ungroup() |>\n    ggplot(aes(reorder(timeSeriescode, not_null, decreasing = T), code, fill = type)) +\n    geom_tile() +\n    scale_y_discrete(labels = NULL) +\n    theme(legend.position = \"bottom\") +\n    labs(\n        x = \"timeSeriesCode\",\n        y = \"station\",\n        title = \"Time series offered by station\"\n    )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nThis shows lots of time series. The first section, which is entirely filled in, represents water-level observations.\nThis makes sense: each station has equipment for measuring water level.\nThe next section which is mostly filled in, contains various time series for water-level predictions: `wlp`;`wlf` for water-level forecasts; and `wlf-spine` for water-level forecasts using the St. Lawrence *prévisions interpolées de niveaux d'eau*.\nThe rest of the plot shows secondary measurements (eg. `wl2` for water level at measurement point 2) and water current data.\n\nTo get a sense of where we can find water-level predictions, let's map the stations and pull in their relevant time series.\nTo do so, we'll use `ggmap` and `ggrepel`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_stations |>\n    mutate(timeSeriescode = if_else(timeSeriescode %in% c(\"wlp\", \"wlf\", \"wlf-vtg\", \"wlf-spine\"), timeSeriescode, NA)) |>\n    group_by(officialName, longitude, latitude) |>\n    summarize(predictionCode = first(timeSeriescode, order_by = desc(timeSeriescode))) |>\n    ggplot() +\n    geom_polygon(data = map_data(\"world\", region = \"Canada\"), aes(x = long, y = lat, group = group, alpha = .1), show.legend = FALSE) +\n    geom_point(\n        aes(x = longitude, y = latitude, color = predictionCode),\n    ) +\n    ggrepel::geom_label_repel(\n        aes(x = longitude, y = latitude, label = officialName, color = predictionCode),\n        max.overlaps = 15,\n        show.legend = FALSE\n    ) +\n    theme(legend.position = \"bottom\") +\n    labs(color = \"Prediction Type\", x = NULL, y = NULL, title = \"Canadian tidal stations by prediction type\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nUsing this plot, we may guess that `wlf` is for freshwater, where the tide is more influenced by meterological events than saltwater stations, which are more influenced by [the various components that create an ocean tide](https://tides.gc.ca/en/tidal-phenomena).\n\n# Does each station offer predictions at the 15-minute grain?\n\nRemember our goal is to match the US data with tidal predictions every 15 minutes for all Canadian stations.\nIf the data aren't provided then we will need to do some front-end work to accommodate null values or change the user's ability to select lower granularities.\n\nTo assess the coverage of 15-minute intervals, I'll write a function that will take arguments from the data frame and build a URL to query the API.\nThen, I'll query each station using that function and extract the results as a data frame for further analysis.\n\n## Constructing the set of stations\n\nFirst, I'll filter to the stations with a listed `timeSeriesCode` among the water-level forecasts and predictions.\n\n## Composing the requests\n\nTo make requests appropriately, we need to mind the API's query limits.\nCHS states that\n>Limits on Requests per client (IP):\n>- Up to 3 requests per second\n>- Up to 30 requests per minute\n>\n>Limits on data per request:\n>-1 minute data – 1 week\n>-3 minute data – 3 weeks\n>-all other lower resolutions, - 1 month\n>\n>If you receive a 429 error from your request it is likely you may be exceeding 1 or more of these limits. If this is the case, please modify your requests to comply with the limits defined above.\n\nIn this case, we can use [`req_throttle`](https://httr2.r-lib.org/reference/req_throttle.html) and set the `capacity` to `30` (per minute).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstation_data <- function(timeSeriescode, id) {\n    base_url |>\n    req_url_path_append(\"stations\") |>\n    req_url_path_append(id) |>\n    req_url_path_append(\"data\") |>\n    req_url_query(!!!list(\n        `time-series-code` = timeSeriescode, # water-level predictions\n        from = \"2025-08-11T00:00:00Z\",\n        to = \"2025-08-12T05:00:00Z\",\n        resolution = \"FIFTEEN_MINUTES\"\n    )) |>\n    req_throttle(capacity = 30)\n}\n```\n:::\n\n\nUsually, if I were writing a function, I wouldn't hardcode what the user may want to change; I'd leave `from`, `to`, and `resolution` undefined so that users could specify how they'd like.\nIn this case, I decided to hardcode them to make the next bit easier.\n`purrr::pmap` takes a dataframe and performs the function on each row, filling arguments by matching names from the function signature to column names in the dataframe.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrequests <- all_stations |>\n    filter(timeSeriescode %in% c(\"wlp\", \"wlf\", \"wlf-spine\")) |>\n    select(\n        id,\n        timeSeriescode\n    ) |>\n    pmap(station_data)\n```\n:::\n\n\nWe now have a list of `httr2_request`s.\nHere are the first 2 elements:\n\n::: {.cell}\n\n```{.r .cell-code}\nrequests[1:2]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n<httr2_request>\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nGET\nhttps://api.iwls-sine.azure.cloud-nuage.dfo-mpo.gc.ca/api/v1/stations/5cebf1e43d0f4a073c4bc45a/data?time-series-code=wlp&from=2025-08-11T00%3A00%3A00Z&to=2025-08-12T05%3A00%3A00Z&resolution=FIFTEEN_MINUTES\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nHeaders:\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• accept: \"application/json\"\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nBody: empty\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nOptions:\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• useragent: \"floatingtrails.dev\"\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nPolicies:\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• throttle_realm: \"api.iwls-sine.azure.cloud-nuage.dfo-mpo.gc.ca\"\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n[[2]]\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n<httr2_request>\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nGET\nhttps://api.iwls-sine.azure.cloud-nuage.dfo-mpo.gc.ca/api/v1/stations/5cebf1e43d0f4a073c4bc3a3/data?time-series-code=wlf&from=2025-08-11T00%3A00%3A00Z&to=2025-08-12T05%3A00%3A00Z&resolution=FIFTEEN_MINUTES\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nHeaders:\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• accept: \"application/json\"\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nBody: empty\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nOptions:\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• useragent: \"floatingtrails.dev\"\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nPolicies:\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• throttle_realm: \"api.iwls-sine.azure.cloud-nuage.dfo-mpo.gc.ca\"\n```\n\n\n:::\n:::\n\n\nSince we specified how to throttle requests for each request, we can now parallelize their execution.\nIf any of them error out---for instance, if this breaks the API's request limit despite the documentation or if any time series are not available even though they were returned by the `stations` endpoint---I'd like to know but also continue the requests.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresponses <- requests |>\n    req_perform_parallel(on_error = \"continue\")\n```\n:::\n\n\n## Extract predictions\nThis returns a list of HTML responses.\nInstead of extracting the data we need from the body, I left this object in memory so that I could work on different ways to extract it.\n`purrr::map` applies functions across a list and returns the same list back, making its output format stable.\nAs much as I like working with dataframes, keeping the data structure nested avoids multiplying the size of the data.\nFor this code block, I commented each line to explain what it does:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidal_data <- responses |>\n    map(resp_body_json) |> # extract body of response, assuming JSON\n    map_depth(.depth = 2, .f = as_tibble_row) |> # convert each element into a list of dataframe rows\n    map(list_rbind) |> # bind the list of dataframe rows into a single dataframe for each element\n    set_names(map(responses, \"url\")) |> # name each element according to its url\n    list_rbind(names_to = \"url\") |> # reduce the list of dataframes into a single dataframe with url as a column\n    mutate(\n        id = str_split_i(url, \"/\", 7), # extract station ID\n        time_series_code = str_split_i(url, \"=|&\", 2), # extract time-series code\n        eventDate = as_datetime(eventDate) # parse datetime\n    )\n```\n:::\n\n\n## Visualize predictions for each station\nLet's look at this first by time-series code so that we can see which time series offer the most stations for each 15-minute block of time.\nIt would be great if one time series, like `wlp` offered total coverage:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidal_data |>\n    group_by(time_series_code, eventDate) |>\n    summarize(observations = n()/nrow(distinct(tidal_data, id))) |>\n    ggplot() +\n    aes(x = eventDate, y = time_series_code, fill = observations) +\n    geom_tile(color = \"gray70\") +\n    scale_fill_continuous(labels = scales::percent, limits = c(0, 1)) +\n    scale_x_datetime(date_labels = \"%H%p\", date_minor_breaks = \"1 hour\") +\n    labs(x = \"Timestamp (August 11-12, 2025)\", y = \"Time-series code\", fill = \"% of stations\\nreporting\") +\n    theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\nOkay, so none of them have 100% coverage.\nBut *between* `wlp`, `wlf`, and `wlf-spine`, can we get predictions for every station?\nTo answer this, let's visualize how many time series are available for each 15-minute block.\nIf any stations are totally null, we'll see that, too.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidal_data |>\n    filter(!is.na(value)) |>\n    group_by(id, eventDate) |>\n    summarize(series_count = n()) |>\n    ggplot() +\n    aes(x = eventDate, y = id, fill = series_count) +\n    geom_tile(color = \"gray70\") +\n    scale_x_datetime(date_labels = \"%H%p\", date_minor_breaks = \"1 hour\") +\n    labs(x = \"Timestamp (August 11-12, 2025)\", y = \"Station\") +\n    theme(legend.position = \"bottom\", axis.text.y = element_blank())\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nVoila---for each station, for every 15 minutes, a prediction.\n\nNext questions:\n\n1. What about current speed and direction?\n2. How far ahead can we query each dataset?\n3. When should we use `wlf` versus `wlp`?\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}