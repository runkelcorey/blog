{
  "hash": "710c0e73aa1708693984db88b93a14cf",
  "result": {
    "markdown": "---\ntitle: A PySpark-native way to do recursion\nsubtitle: \"Part 1: the Problem\"\nauthor: corey\ndate: 2024-04-21\ncategories:\n  - data\n  - spark\n  - python\neditor_options: \n  markdown: \n    wrap: sentence\n---\n\nI find lots of situations in my work where recursion in Spark dataframes would be useful.\nRecently, I needed to define a column with values that were based on each following value, recursively.\nThat's to say, each row could only be computed after the row before it.\n\n## a simple example\n\nTo make this example concrete, I'll use the Fibonacci sequence:\n\n$$\n0, 1, 1, 2, 3, 5, 8, 11, \\dots\n$$\n\n\nGenerating the sequence is really easy: the first 2 values are $0$ and $1$, respectively, and each number $fibonacci(n)$ in the sequence depends on the last 2 values.\n\n$$\nfibonacci(0) = 0\n$$\n\n$$\nfibonacci(1) = 1\n$$\n\n$$\nfibonacci(n) = fibonacci(n-1) + fibonacci(n-2)\n$$\n\n\nPlus, the sequence is equally easy to code:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\ndef fibonacci(n: int) -> int:\n  if n == 0:\n    f = 0\n  elif n == 1:\n    f = 1\n  else:\n    f = fibonacci(n - 1) + fibonacci(n - 2)\n  return f\n```\n:::\n\n\nRunning this code yields the same sequence we started with:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n[fibonacci(n) for n in range(0, 8)]\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n[0, 1, 1, 2, 3, 5, 8, 13]\n```\n:::\n:::\n\n\n## the problem\n\nBut, try to calculate this on a Spark dataframe and you'll quickly encounter a problem.\nThe features of distributed computing---that it calculates each row of the dataframe simultaneously---presents as a bug here.\nCalculating `fibonacci(n)` requires calculating `fibonacci(n-1)` and `fibonacci(n-2)` and `fibonacci(n-3)` et cetera, and Spark just doesn't like that.\n\nI use the fibonacci sequence as an example because it's simple to code and even to calculate by hand, but the approach to calculate it in Spark is the same as much more interesting problems.\nFor example, my current work requires me to calculate the Poisson cumulative distribution function for various values of $\\lambda$ so that we can turn point forecasts of discrete variables into distrubtional forecasts at massive scale.\n\nSo, how can we calculate a fibonacci sequence in Spark?\nOne thing we **can't** do is use window functions.\nThough windowing takes into account previous values, it takes them into account statically.\nTry writing this using window functions and you'll immediately see a problem:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n(\n  df\n  .withColumn(\n    \"Fibonacci\",\n    f.lag(\"Fibonacci\").over(w.Window.orderBy(\"Fibonacci\")) + f.lag(\"Fibonacci\", offset = 2).over(w.Window.orderBy(\"Fibonacci\"))\n  )\n)\n```\n:::\n\n\nEither you need to have `Fibonacci` already populated or you need to populate it using a non-recursive function, but what would that be?\nI'll explore that in my next post.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}